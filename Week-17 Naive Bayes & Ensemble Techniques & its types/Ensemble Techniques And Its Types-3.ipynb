{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Random Forest Regressor?\n",
    "\n",
    "Random Forest Regressor is an ensemble learning method that constructs multiple decision trees during training and averages their predictions to produce a more accurate and stable output. It is a type of bagging technique that improves performance and reduces overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "\n",
    "Random Forest Regressor reduces overfitting by:\n",
    "- Training multiple decision trees on different bootstrap samples of the dataset.\n",
    "- Using feature randomness when splitting nodes to ensure diverse trees.\n",
    "- Averaging predictions, which helps smooth out individual tree errors and reduces variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "\n",
    "For regression tasks, Random Forest Regressor aggregates predictions by averaging the outputs of all decision trees in the ensemble, leading to a more stable and accurate prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "\n",
    "Some key hyperparameters include:\n",
    "- `n_estimators`: Number of trees in the forest.\n",
    "- `max_depth`: Maximum depth of each tree.\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: Minimum number of samples required to be a leaf node.\n",
    "- `max_features`: Number of features to consider when looking for the best split.\n",
    "- `bootstrap`: Whether bootstrap samples are used when building trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "- **Decision Tree Regressor**: A single tree that learns patterns from data but is prone to overfitting.\n",
    "- **Random Forest Regressor**: An ensemble of multiple decision trees that reduces variance and improves generalization by averaging multiple predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces overfitting compared to single decision trees.\n",
    "- Handles large datasets effectively.\n",
    "- Works well with both numerical and categorical data.\n",
    "- Can capture non-linear relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires more computational resources than a single decision tree.\n",
    "- Less interpretable than individual decision trees.\n",
    "- Training can be slower for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Q7. What is the output of Random Forest Regressor?\n",
    "\n",
    "The output of Random Forest Regressor is a numerical value, which is the averaged prediction from all decision trees in the ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "## Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "\n",
    "Yes, Random Forest can be used for classification tasks. In that case, it is referred to as **Random Forest Classifier**, where the final prediction is determined by majority voting among the decision trees rather than averaging their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
