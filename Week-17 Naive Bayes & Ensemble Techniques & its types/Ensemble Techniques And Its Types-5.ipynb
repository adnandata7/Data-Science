{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "### Design a pipeline that includes the following steps:\n",
    "1. **Use an automated feature selection method to identify the important features in the dataset**\n",
    "2. **Create a numerical pipeline that includes the following steps:**\n",
    "   - Impute the missing values in the numerical columns using the mean of the column values.\n",
    "   - Scale the numerical columns using standardization.\n",
    "3. **Create a categorical pipeline that includes the following steps:**\n",
    "   - Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "   - One-hot encode the categorical columns.\n",
    "4. **Combine the numerical and categorical pipelines using a ColumnTransformer**\n",
    "5. **Use a Random Forest Classifier to build the final model**\n",
    "6. **Evaluate the accuracy of the model on the test dataset**\n",
    "\n",
    "### Code Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7542\n",
      "Cross-validation Accuracy: 0.7683 ± 0.0179\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "# Drop columns not needed\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numerical_features),\n",
    "    ('cat', cat_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Feature selection using RandomForestClassifier\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f'Model Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Cross-validation Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation and Improvements:\n",
    "- The accuracy score gives us an idea of model performance. If it is low, we can try:\n",
    "  - Tuning hyperparameters of the Random Forest Classifier.\n",
    "  - Using different feature selection techniques.\n",
    "  - Trying alternative imputations for missing values.\n",
    "  - Experimenting with other classifiers like XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "### Code Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Define classifiers\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Create voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf_clf), (\"lr\", log_reg)], voting=\"hard\")\n",
    "\n",
    "# Train and evaluate model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation and Improvements:\n",
    "- A voting classifier combines the strengths of both models to improve overall accuracy.\n",
    "- To further enhance performance:\n",
    "  - Use soft voting instead of hard voting for probabilistic predictions.\n",
    "  - Add more diverse models, such as SVM or KNN.\n",
    "  - Perform hyperparameter tuning on individual classifiers.\n",
    "\n",
    "This pipeline ensures robustness and improved classification accuracy using ensemble learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
