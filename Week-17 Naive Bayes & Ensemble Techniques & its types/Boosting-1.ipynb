{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is boosting in machine learning?\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners to create a strong learner by reducing bias and variance. It sequentially trains weak models, focusing more on the misclassified instances in each iteration.\n",
    "\n",
    "### Q2. What are the advantages and limitations of using boosting techniques?\n",
    "**Advantages:**\n",
    "- Improves accuracy and reduces bias\n",
    "- Handles missing data well\n",
    "- Works well with structured data\n",
    "- Can be used with different weak learners\n",
    "\n",
    "**Limitations:**\n",
    "- Sensitive to noisy data\n",
    "- Computationally expensive\n",
    "- Can lead to overfitting if not tuned properly\n",
    "\n",
    "### Q3. Explain how boosting works.\n",
    "Boosting works by training models sequentially, where each new model corrects errors made by previous ones. It assigns weights to samples, increasing the importance of misclassified instances. The final prediction is a weighted sum of all weak learners.\n",
    "\n",
    "### Q4. What are the different types of boosting algorithms?\n",
    "- AdaBoost (Adaptive Boosting)\n",
    "- Gradient Boosting (GBM)\n",
    "- XGBoost (Extreme Gradient Boosting)\n",
    "- LightGBM (Light Gradient Boosting Machine)\n",
    "- CatBoost (Categorical Boosting)\n",
    "\n",
    "### Q5. What are some common parameters in boosting algorithms?\n",
    "- **Learning rate:** Controls step size in updating weights\n",
    "- **Number of estimators:** Number of weak learners\n",
    "- **Max depth:** Maximum depth of decision trees\n",
    "- **Min samples split:** Minimum samples needed to split a node\n",
    "- **Subsample:** Fraction of data used for training each iteration\n",
    "\n",
    "### Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "Boosting assigns weights to weak learners and combines their predictions using a weighted sum or voting mechanism. The weak learners focus more on misclassified samples in each iteration, improving overall accuracy.\n",
    "\n",
    "### Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "AdaBoost (Adaptive Boosting) is a boosting algorithm that assigns weights to training samples. Initially, all samples have equal weight. After each iteration, misclassified samples receive higher weights, forcing the next weak learner to focus on them. The final model combines predictions from all weak learners.\n",
    "\n",
    "### Q8. What is the loss function used in AdaBoost algorithm?\n",
    "AdaBoost minimizes the exponential loss function, which penalizes misclassified samples exponentially, making the model focus on hard-to-classify instances.\n",
    "\n",
    "### Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "AdaBoost updates weights as follows:\n",
    "1. Assigns higher weights to misclassified samples.\n",
    "2. Reduces weights of correctly classified samples.\n",
    "3. Normalizes weights to sum up to 1.\n",
    "This ensures that the next weak learner focuses more on hard-to-classify samples.\n",
    "\n",
    "### Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "Increasing the number of estimators generally improves accuracy up to a certain point. However, too many estimators can lead to overfitting, making the model less generalizable.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
