{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics that describes how to update our beliefs based on new evidence. It provides a way to calculate the probability of a hypothesis given observed data and is widely used in various fields, including machine learning, medical diagnosis, and spam filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is mathematically expressed as:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$\n",
    "\n",
    "where:\n",
    "- \\( P(A|B) \\) is the probability of event \\( A \\) occurring given that event \\( B \\) has occurred.\n",
    "- \\( P(B|A) \\) is the probability of event \\( B \\) occurring given that event \\( A \\) has occurred.\n",
    "- \\( P(A) \\) is the prior probability of event \\( A \\).\n",
    "- \\( P(B) \\) is the prior probability of event \\( B \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in various real-world applications, including:\n",
    "- **Spam Filtering**: Determining whether an email is spam based on the presence of certain words.\n",
    "- **Medical Diagnosis**: Estimating the probability of a disease given test results.\n",
    "- **Machine Learning (Naive Bayes Classifier)**: Predicting classes in classification problems.\n",
    "- **Risk Assessment**: Evaluating the likelihood of financial risks or fraud detection.\n",
    "- **Natural Language Processing**: Text classification, sentiment analysis, and document categorization.\n",
    "\n",
    "---\n",
    "\n",
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is a direct application of conditional probability. Conditional probability is defined as:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "Bayes' theorem extends this by expressing \\( P(A|B) \\) in terms of \\( P(B|A) \\), allowing us to update prior knowledge based on new data.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "There are different types of Naive Bayes classifiers, and the choice depends on the nature of the features:\n",
    "\n",
    "- **Gaussian Naive Bayes**: Used when the features are continuous and assumed to follow a normal distribution.\n",
    "- **Multinomial Naive Bayes**: Suitable for text classification problems where features represent word counts or frequencies.\n",
    "- **Bernoulli Naive Bayes**: Best for binary feature data, such as whether a word appears in a document (yes/no).\n",
    "- **Complement Naive Bayes**: A variant of Multinomial Naive Bayes, designed to handle class imbalance better.\n",
    "\n",
    "Choosing the right type depends on the data distribution and feature representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Q6. Assignment:\n",
    "\n",
    "You have a dataset with two features, \\( X_1 \\) and \\( X_2 \\), and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\). The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "\n",
    "### Solution:\n",
    "\n",
    "#### Step 1: Compute Prior Probabilities\n",
    "Since both classes have equal prior probability:\n",
    "\n",
    "$$ P(A) = P(B) = 0.5 $$\n",
    "\n",
    "#### Step 2: Compute Likelihoods\n",
    "Using the Naive Bayes assumption (features are independent given the class), we calculate:\n",
    "\n",
    "- \\( P(X_1 = 3 | A) = \\frac{4}{3+3+4} = \\frac{4}{10} = 0.4 \\)\n",
    "- \\( P(X_2 = 4 | A) = \\frac{3}{4+3+3+3} = \\frac{3}{13} \\approx 0.23 \\)\n",
    "\n",
    "- \\( P(X_1 = 3 | B) = \\frac{1}{2+2+1} = \\frac{1}{5} = 0.2 \\)\n",
    "- \\( P(X_2 = 4 | B) = \\frac{3}{2+2+2+3} = \\frac{3}{9} = 0.33 \\)\n",
    "\n",
    "#### Step 3: Compute Posterior Probabilities\n",
    "\n",
    "Using Bayes' rule:\n",
    "\n",
    "For Class A:\n",
    "\n",
    "$$ P(A | X_1=3, X_2=4) \\propto P(X_1=3 | A) P(X_2=4 | A) P(A) $$\n",
    "\n",
    "$$ = 0.4 \\times 0.23 \\times 0.5 = 0.046 $$\n",
    "\n",
    "For Class B:\n",
    "\n",
    "$$ P(B | X_1=3, X_2=4) \\propto P(X_1=3 | B) P(X_2=4 | B) P(B) $$\n",
    "\n",
    "$$ = 0.2 \\times 0.33 \\times 0.5 = 0.033 $$\n",
    "\n",
    "#### Step 4: Determine Prediction\n",
    "Since \\( P(A | X_1=3, X_2=4) > P(B | X_1=3, X_2=4) \\), Naive Bayes predicts that the new instance belongs to **Class A**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
