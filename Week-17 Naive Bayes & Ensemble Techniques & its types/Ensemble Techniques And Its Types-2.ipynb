{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by training multiple models on different bootstrap samples of the dataset and averaging their predictions (for regression) or using majority voting (for classification). This process reduces variance while maintaining low bias, making the model more robust and less prone to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "**Advantages:**\n",
    "- Using complex models (e.g., deep decision trees) benefits from bagging by reducing variance.\n",
    "- Simpler models can improve interpretability and efficiency while still gaining robustness.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Too complex base learners may still overfit if ensemble size is not large enough.\n",
    "- Too simple base learners may result in high bias, leading to underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "The choice of base learner determines how bagging impacts bias and variance:\n",
    "- **High variance models (e.g., deep decision trees)**: Bagging reduces variance significantly, leading to better generalization.\n",
    "- **High bias models (e.g., shallow trees, linear models)**: Bagging has a limited effect, as it does not reduce bias much, potentially leading to underfitting.\n",
    "\n",
    "The ideal base learner is one with high variance and low bias, where bagging can effectively reduce variance without introducing too much bias.\n",
    "\n",
    "---\n",
    "\n",
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "Yes, bagging can be used for both classification and regression:\n",
    "- **Classification**: Uses majority voting among base learners.\n",
    "- **Regression**: Uses averaging of predictions to produce a smoother output.\n",
    "\n",
    "In classification, bagging helps reduce misclassification errors, while in regression, it smooths predictions and reduces noise.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\n",
    "- A larger ensemble generally improves performance but has diminishing returns beyond a certain point.\n",
    "- Too few models may not provide sufficient variance reduction.\n",
    "- Typically, 50-200 models work well in practice, but the optimal number depends on dataset size and computational resources.\n",
    "\n",
    "---\n",
    "\n",
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\n",
    "A real-world application of bagging is in **medical diagnosis systems**. For example, Random Forest (a bagging-based method) is used to detect diseases from medical imaging by aggregating multiple decision trees trained on different subsets of patient data. This enhances reliability and reduces overfitting, leading to more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
