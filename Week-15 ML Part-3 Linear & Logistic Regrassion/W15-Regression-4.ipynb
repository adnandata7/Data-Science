{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a regularization term, which penalizes the absolute value of the coefficients. This results in some coefficients being exactly zero, effectively selecting a subset of the features. The cost function for Lasso Regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\theta_j|\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter and $\\theta_j$ represents the coefficients.\n",
    "\n",
    "**Key differences from other regression techniques:**\n",
    "- Unlike ordinary least squares (OLS), which does not use regularization, Lasso applies L1 regularization.\n",
    "- Lasso can perform feature selection by shrinking some coefficients to exactly zero, while Ridge Regression (with L2 regularization) shrinks coefficients but does not set them exactly to zero.\n",
    "\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is that it can **automatically select important features** by shrinking irrelevant feature coefficients to exactly zero. This makes it particularly useful in scenarios with a large number of features, where many may be irrelevant or redundant. \n",
    "\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "In Lasso Regression:\n",
    "- Coefficients that are exactly zero indicate that the corresponding features have been excluded from the model.\n",
    "- Non-zero coefficients indicate the selected features that contribute to the prediction. The magnitude of these coefficients reflects the strength and direction of the relationship between the feature and the target variable.\n",
    "- Larger values of $\\lambda$ result in more coefficients being set to zero.\n",
    "\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The key tuning parameter in Lasso Regression is **$\\lambda$ (the regularization parameter)**. It controls the strength of the penalty on the absolute values of the coefficients:\n",
    "\n",
    "- **Small $\\lambda$**: Little to no regularization, the model will behave more like ordinary least squares, keeping most coefficients non-zero.\n",
    "- **Large $\\lambda$**: Stronger regularization, resulting in more coefficients being set to zero, which leads to simpler models (with fewer features).\n",
    "\n",
    "Other tuning parameters, depending on the implementation, might include the **maximum number of iterations** or **convergence tolerance** for the optimization algorithm.\n",
    "\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression, in its basic form, is a linear model. However, it can be extended to handle non-linear relationships by **transforming the input features**. One common approach is to use **polynomial features** or **interaction terms** as inputs to the Lasso model. Alternatively, **kernel methods** or **basis function expansions** can be used to allow the Lasso model to capture non-linear relationships.\n",
    "\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "- **Penalty Type**: \n",
    "  - Ridge Regression uses L2 regularization, which penalizes the sum of the squared coefficients: $\\lambda \\sum_{j=1}^{p} \\theta_j^2$.\n",
    "  - Lasso Regression uses L1 regularization, which penalizes the sum of the absolute values of the coefficients: $\\lambda \\sum_{j=1}^{p} |\\theta_j|$.\n",
    "  \n",
    "- **Feature Selection**:\n",
    "  - Ridge Regression shrinks the coefficients but does not set any of them to exactly zero, meaning all features are retained.\n",
    "  - Lasso Regression shrinks some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "- **Use Cases**:\n",
    "  - Ridge is preferred when multicollinearity is an issue and all features are expected to be useful.\n",
    "  - Lasso is preferred when there are many features and the goal is to select the most important ones.\n",
    "\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent by selecting one of the correlated features while shrinking others to zero. Since it can drive some coefficients to zero, Lasso helps in reducing redundancy among highly correlated features, unlike Ridge Regression, which tends to keep all features.\n",
    "\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "The optimal value of the regularization parameter $\\lambda$ is typically chosen through **cross-validation**. The steps are:\n",
    "\n",
    "1. Split the dataset into training and validation sets.\n",
    "2. Train the Lasso model for different values of $\\lambda$ (often on a logarithmic scale).\n",
    "3. Evaluate the model's performance on the validation set and choose the $\\lambda$ value that minimizes the validation error (or another relevant metric).\n",
    "4. Retrain the model on the entire training set using the selected $\\lambda$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
