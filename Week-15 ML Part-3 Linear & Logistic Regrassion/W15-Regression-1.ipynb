{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27598cb5-c2d8-4fb4-b35c-e85209d5c599",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Simple linear regression involves one independent variable and one dependent variable. It models the relationship between these two variables by fitting a linear equation. The general form of a simple linear regression equation is:\n",
    "\\[ y = \\beta_0 + \\beta_1x + \\epsilon \\]\n",
    "\n",
    "- **Example:** Predicting a person's weight (`y`) based on their height (`x`).\n",
    "\n",
    "Multiple linear regression, on the other hand, involves two or more independent variables. The relationship is modeled using a linear equation that fits multiple predictors. The general form of a multiple linear regression equation is:  \n",
    "$$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n + \\epsilon $$\n",
    "\n",
    "- **Example:** Predicting a person's weight (`y`) based on their height (`x_1`), age (`x_2`), and exercise frequency (`x_3`).\n",
    "\n",
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The main assumptions of linear regression are:\n",
    "\n",
    "1. **Linearity:** The relationship between the independent and dependent variables is linear.\n",
    "2. **Independence:** The observations are independent of each other.\n",
    "3. **Homoscedasticity:** The residuals (errors) have constant variance at every level of the independent variable.\n",
    "4. **Normality:** The residuals are normally distributed.\n",
    "5. **No Multicollinearity:** In multiple linear regression, the independent variables are not highly correlated with each other.\n",
    "\n",
    "To check whether these assumptions hold:\n",
    "\n",
    "- **Linearity:** Use scatterplots of the independent variables against the dependent variable.\n",
    "- **Independence:** This can be checked by plotting residuals against time or sequence if data is time-series. Durbin-Watson test can also be used.\n",
    "- **Homoscedasticity:** Plot residuals vs. predicted values or independent variables.\n",
    "- **Normality:** Use Q-Q plots or histograms of residuals.\n",
    "- **No Multicollinearity:** Check the Variance Inflation Factor (VIF) or correlation matrix.\n",
    "\n",
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "In a linear regression model:\n",
    "- The **intercept** $(\\beta_0)$ is the predicted value of the dependent variable when all independent variables are zero.\n",
    "- The **slope** $ (\\beta_1, \\beta_2) $, etc.) represents the change in the dependent variable for a one-unit change in the respective independent variable.\n",
    "\n",
    "**Example:** If we are predicting salary based on years of experience:\n",
    "\n",
    "- **Intercept:** The starting salary when experience is zero years.\n",
    "- **Slope:** The increase in salary for each additional year of experience.\n",
    "\n",
    "Suppose the equation is:\n",
    "$ \\text{Salary} = 30,000 + 5,000 \\times (\\text{Years of Experience})$\n",
    "- The intercept is 30,000, indicating the starting salary.\n",
    "- The slope is 5,000, meaning each additional year of experience increases the salary by $5,000.\n",
    "\n",
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function by iteratively adjusting the model parameters (weights). It involves calculating the gradient of the cost function with respect to the parameters and moving the parameters in the direction of the negative gradient to reduce the error.\n",
    "\n",
    "In machine learning, gradient descent is commonly used in training models, particularly in linear regression, logistic regression, and neural networks.\n",
    "\n",
    "- **Steps:**\n",
    "  1. Initialize parameters (weights) randomly.\n",
    "  2. Compute the gradient of the cost function.\n",
    "  3. Update the parameters in the opposite direction of the gradient.\n",
    "  4. Repeat until convergence.\n",
    "\n",
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Multiple linear regression models the relationship between one dependent variable and two or more independent variables. It extends simple linear regression, which only involves one independent variable.\n",
    "\n",
    "- **Equation for Simple Linear Regression:**\n",
    "  $ y = \\beta_0 + \\beta_1x + \\epsilon $\n",
    "\n",
    "- **Equation for Multiple Linear Regression:**\n",
    "  $ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n + \\epsilon $\n",
    "\n",
    "The key difference is that multiple linear regression can capture the effect of several predictors on the dependent variable, allowing for more complex models and potentially better predictions.\n",
    "\n",
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, making it difficult to isolate the effect of each variable on the dependent variable. This can lead to inflated standard errors and unreliable estimates of the coefficients.\n",
    "\n",
    "- **Detection:**\n",
    "  - **Variance Inflation Factor (VIF):** A high VIF (> 10) indicates multicollinearity.\n",
    "  - **Correlation Matrix:** A high correlation coefficient (close to 1 or -1) between independent variables suggests multicollinearity.\n",
    "\n",
    "- **Addressing Multicollinearity:**\n",
    "  - **Remove one of the correlated variables.**\n",
    "  - **Combine correlated variables into a single predictor.**\n",
    "  - **Use regularization techniques like Ridge or Lasso regression.**\n",
    "\n",
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Polynomial regression is a type of regression that models the relationship between the dependent and independent variables as an nth-degree polynomial. It is used when the relationship between the variables is not linear.\n",
    "\n",
    "- **Equation for Polynomial Regression:**\n",
    "  $ y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\cdots + \\beta_nx^n + \\epsilon $\n",
    "\n",
    "The key difference from linear regression is that in polynomial regression, the independent variable is raised to a power greater than one, allowing the model to fit nonlinear relationships.\n",
    "\n",
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Advantages of Polynomial Regression:**\n",
    "- Can model nonlinear relationships between variables.\n",
    "- Provides a better fit for data that exhibits curvature.\n",
    "\n",
    "**Disadvantages of Polynomial Regression:**\n",
    "- Can lead to overfitting, especially with high-degree polynomials.\n",
    "- More complex to interpret than linear regression.\n",
    "\n",
    "**When to Use Polynomial Regression:**\n",
    "- When the relationship between the independent and dependent variables shows curvature.\n",
    "- When linear regression does not provide a good fit and a higher-degree polynomial better captures the trend in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711a2eb-f371-4a57-bd2e-a6d52347db73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
