{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1. What is the KNN algorithm?**\n",
    "**Answer:**  \n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It works by finding the 'K' training data points closest to a given test point and making predictions based on the majority label (for classification) or the average value (for regression) of those neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How do you choose the value of K in KNN?**\n",
    "**Answer:**  \n",
    "The value of K is typically chosen using:\n",
    "- **Cross-validation**, to find the K that gives the best performance.\n",
    "- **Odd values** are often used in classification to avoid ties.\n",
    "- A **small K** can lead to overfitting, while a **large K** may cause underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. What is the difference between KNN classifier and KNN regressor?**\n",
    "**Answer:**  \n",
    "- **KNN Classifier**: Predicts the class label based on the majority vote from the nearest neighbors.\n",
    "- **KNN Regressor**: Predicts the output value by averaging the values of the nearest neighbors.\n",
    "- The choice depends on whether the target variable is **categorical (classifier)** or **continuous (regressor)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. How do you measure the performance of KNN?**\n",
    "**Answer:**  \n",
    "- **Classification**: Accuracy, Precision, Recall, F1-Score, Confusion Matrix.\n",
    "- **Regression**: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), RÂ² Score.\n",
    "- **Cross-validation** is also commonly used for reliable performance estimation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. What is the curse of dimensionality in KNN?**\n",
    "**Answer:**  \n",
    "The curse of dimensionality refers to the problem where the feature space becomes sparse as the number of dimensions increases, making distance calculations less meaningful. This can degrade the performance of KNN, as it relies heavily on distance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. How do you handle missing values in KNN?**\n",
    "**Answer:**  \n",
    "- **Remove** rows or columns with missing values (if minimal).\n",
    "- **Impute** missing values using strategies like mean, median, or KNN imputation.\n",
    "- **Use KNN Imputer**: A variation that fills in missing values based on the closest neighbors with available values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "**Answer:**  \n",
    "- **KNN Classifier** works well when data points naturally cluster around specific classes.\n",
    "- **KNN Regressor** is used for problems where the target is continuous and behaves similarly among neighboring values.\n",
    "- The **performance depends on the problem**, data distribution, and feature space structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "**Answer:**  \n",
    "**Strengths:**\n",
    "- Simple and intuitive.\n",
    "- No training phase (lazy learner).\n",
    "- Works well with small datasets.\n",
    "\n",
    "**Weaknesses:**\n",
    "- Computationally expensive during prediction.\n",
    "- Sensitive to irrelevant features and the scale of data.\n",
    "- Poor with high-dimensional or imbalanced data.\n",
    "\n",
    "**Solutions:**\n",
    "- Use **feature selection** and **scaling**.\n",
    "- Apply **dimensionality reduction (PCA)**.\n",
    "- Use **approximate nearest neighbor** methods to speed up prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "**Answer:**  \n",
    "- **Euclidean Distance**: Measures the straight-line (L2 norm) distance between two points.  \n",
    "  $$\n",
    "  \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Manhattan Distance**: Measures the absolute sum of differences (L1 norm).  \n",
    "  $$\n",
    "  \\text{Manhattan Distance} = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "  $$\n",
    "\n",
    "- Choice depends on the problem context; Manhattan is better for grid-like or sparse structures.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q10. What is the role of feature scaling in KNN?**\n",
    "**Answer:**  \n",
    "KNN is distance-based, so features with larger ranges can dominate distance calculations. Feature scaling (like **Min-Max scaling** or **Standardization**) ensures all features contribute equally, improving model performance and accuracy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
