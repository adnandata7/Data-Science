{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?**\n",
    "\n",
    "**Answer:**  \n",
    "- **Euclidean Distance** measures the straight-line distance between two points:\n",
    "  $$\n",
    "  d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Manhattan Distance** sums the absolute differences:\n",
    "  $$\n",
    "  d = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "  $$\n",
    "\n",
    "**Impact on performance:**\n",
    "- **Euclidean** is sensitive to large differences in individual dimensions and works better when data is dense and continuous.\n",
    "- **Manhattan** is better for high-dimensional or sparse data (like text) and grid-based structures (e.g., city blocks).\n",
    "- The choice of metric can significantly impact neighbor selection and, therefore, model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?**\n",
    "\n",
    "**Answer:**  \n",
    "- Use **Cross-Validation** (e.g., k-fold CV) to test different values of **k** and select the one that gives the best performance on the validation set.\n",
    "- Common techniques:\n",
    "  - **Grid Search** for hyperparameter tuning.\n",
    "  - **Elbow Method** to plot performance metrics vs. different k values and choose the \"elbow\" point.\n",
    "  - **Leave-one-out cross-validation (LOOCV)** for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?**\n",
    "\n",
    "**Answer:**  \n",
    "- The **distance metric** determines how neighbors are selected.\n",
    "- **Euclidean** is sensitive to feature scales and outliers.\n",
    "- **Manhattan** is better when features are not normally distributed or when dealing with high-dimensional data.\n",
    "- Choose **Euclidean** when features are continuous and well-scaled.  \n",
    "- Choose **Manhattan** for high-dimensional, sparse, or grid-like data.\n",
    "\n",
    "Other distance metrics:\n",
    "\n",
    "- **Minkowski Distance** (generalized form):\n",
    "\n",
    "  $$\n",
    "  d = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}\n",
    "  $$\n",
    "\n",
    "  Special cases:\n",
    "\n",
    "  $$\n",
    "  p = 1 \\Rightarrow \\text{Manhattan Distance}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  p = 2 \\Rightarrow \\text{Euclidean Distance}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?**\n",
    "\n",
    "**Answer:**  \n",
    "**Common hyperparameters:**\n",
    "- **k (number of neighbors)** – Controls model complexity.\n",
    "- **Distance metric** – Affects how similarity is measured.\n",
    "- **Weight function** – `'uniform'` (equal weight) or `'distance'` (closer neighbors have more influence).\n",
    "\n",
    "**Tuning methods:**\n",
    "- **Grid Search / Random Search** with cross-validation.\n",
    "- Use **scikit-learn’s GridSearchCV** or **RandomizedSearchCV**.\n",
    "- Perform **feature scaling** before tuning to ensure fair distance measurement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?**\n",
    "\n",
    "**Answer:**  \n",
    "- Larger training sets improve generalization and performance, but increase **computational cost** at prediction time.\n",
    "- KNN is a **lazy learner**: no training phase, but slow during inference.\n",
    "- Too small a dataset → **underfitting**; too large → **slow prediction**.\n",
    "\n",
    "**Optimization techniques:**\n",
    "- Use **approximate nearest neighbor algorithms** (e.g., KD-Tree, Ball Tree, Locality Sensitive Hashing).\n",
    "- **Data sampling**: Use representative subsets.\n",
    "- Apply **dimensionality reduction** (e.g., PCA) to reduce feature space and training time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?**\n",
    "\n",
    "**Answer:**  \n",
    "**Drawbacks:**\n",
    "- **Computational inefficiency** during prediction.\n",
    "- **Sensitive to irrelevant features and outliers**.\n",
    "- **Struggles with high-dimensional data** (curse of dimensionality).\n",
    "- **Requires feature scaling** to work correctly.\n",
    "\n",
    "**Solutions:**\n",
    "- **Normalize or standardize** features.\n",
    "- Use **feature selection** or **dimensionality reduction** (PCA, t-SNE).\n",
    "- Use **distance-weighted KNN**.\n",
    "- Apply **efficient search techniques** (KD-Trees, Ball Trees).\n",
    "- Use **hybrid models** or **ensemble methods** if needed.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
